# Resuming Hyperbrain

These are some observations on returning to the Hyperbrain project.

## Data

The original `vsm.Corpus` object was prepared on February 26, 2017 and contains 1,882,140 abstracts. Topic models of `K={20, 40, 60, 80, 500}` were trained on a corpus stopped with a low frequency of 668 and high frequency of 140,814, plus the english nltk list.

*Current limitations:* The words in the Corpus object are stored as `<U273` numpy types, rather than native python objects. This will be automatically fixed on the next re-train.



## Software
The original models load under both Python 2 and 3 topic explorers.

The Hyperbrain viewer, which exists in the [JaimieMurdock/hyperbrain](https://github.com/JaimieMurdock/hyperbrain) repository lacks significant documentation. It is only Python 2 compatible.

- `images.py` downloads all the brain slice imagery from Allen Brain Institute.
- `serve.py` is the server for the view.
- `proxy.py` makes a WSGI proxy server to access the Topic Explorer.

*Current limitations:* 
- Needs documentation.
- Run `futurize` to get it into Python 3.

### Getting `hyperbrain` running:
1. `git clone git@github.com:JaimieMurdock/hyperbrain.git`
2. `cd hyperbrain`
3. `python setup.py develop`
4. `cd hyperbrain`
5. `python images.py`
6. `python serve.py /var/inphosemantics/hyperbrain-may/models/data-freq5-nltk-en-freq668-N140814.npz`



# Code observations
## Some notes on the Hyperbrain
Why use `hyperbrain.proxy` when we have access to `topicexplorer.serve.Application`?

The server loads the Corpus object only - can't reuse data.ini from the topicexplorer.

The corpus objects do not have a `doi` or `title` field yet, so there are some errros still.



## Notes on Topic Explorer scaling issues
Perhaps having a regular topic explorer is not the most important part of doing this project, but rather all the meta-analyses that they enable. Getting a performant model on a corpus this large is a significant information retrieval issue - we don't store info into an index or database.

The Handian is about 19,000 documents.
The Old Bailey has about 180,000 trials.
This is about 1,900,000 abstracts. It's another order of magnitude.


# Gameplan
1. Add DOI and Titles using `topicexplorer metadata` functions.
2. Convert words in `data-freq5-....npz` to `np.object`.
3. Launch notebook and get list of top words in each of 500 topics.
4. Send preliminary topic list to Franco. Ask for DOIs of papers to search.

BONUS: Figure out why corpus load is stalling and requires a `Ctrl+C`.

## Rebuild WoS data
1. Figure out data enclave logins again.
2. Find SQL scripts, attempt to put them into `git` version control.
3. Update dataset, ensure that we have intersection with DOIs only. Better ID than WOS ID.

## Create author graph
1. Query for author-WOS_id pairs.
2. Generate for each author:
   - list of works
   - topic distribution based on works
   - list of co-authors (with frequency?)



